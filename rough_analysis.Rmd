

Analysis 1: 

## Restatement of the Problem 
Estimate how SalesPrice of the house is related to the square footage of the living area of the house & if SalesPrice (and its relationship to square footage) depends on which neighborhood the house is located in. We are only interested at looking at houses in the NAmes, Edwards & BrkSide neighborhoods. 
This relationship can be visualized with the equation: 

SalePrice ~ SqFt (GrLIvArea) + Neighborhood + GrLIvArea:Neighborhood


Pulling the data into R and performing some rudimentary cleaning; 

```{r include=FALSE}
library(tidyverse)
library(plotly)
library(car)
library(caret)
library(olsrr)
library(corrplot)
```

Defining functions for future use, I didn't know where to put this: 

```{r RMSE function}
ModelRMSE <- function(res){
  retRMSE <- sqrt(mean(res^2))
  return(retRMSE)
}
```


```{r CVPress function}
CVPress <- function(input_model){
  pred_res = resid(input_model)/(1 - lm.influence(input_model)$hat)
  press <- sum(pred_res^2)
  return(press)
}
```


```{r}
housing = read.csv(file = "train.csv")
test_housing = read.csv(file = "test.csv")

numeric_housing = housing %>% select_if(is.numeric)
a1_set = housing %>% select(SalePrice, Neighborhood, GrLivArea) %>% filter( Neighborhood == c("NAmes", "Edwards", "BrkSide"))
```

We can take a look at the dataset and visualize what the datalooks like with some cursory visuals. During this step, we want to analyze any heavy correlation that the dataset might have, and account for it in any of our developing models;  

```{r}
a1_set %>% filter(Neighborhood == "NAmes") %>% ggplot(aes(x = (GrLivArea), y = SalePrice/1000)) + geom_point() + ggtitle("NAmes Neighborhood Price of Home vs. Square Footage")
```

```{r}

a1_set %>% ggplot(aes(x = (GrLivArea), y = SalePrice/1000, color = Neighborhood)) + geom_point() + stat_smooth(method = "lm")

a1_set %>% ggplot(aes(x = SalePrice/10000, fill = Neighborhood)) + geom_histogram(bins =20)

a1_set %>% ggplot(aes(x = GrLivArea, fill = Neighborhood)) + geom_histogram(bins =20)

```
The histograms show both GrLivArea and Sales Price having a right skew distribution. We will examine different log transformations to see if we can obtain a more normal distribution suitable for our model. 

## Log Transforms 
```{r}
# histograms with log transform on each individual variables
train_a1 %>% ggplot(aes(x = log(SalePrice/10000), fill = Neighborhood)) + geom_histogram(bins =20)

train_a1 %>% ggplot(aes(x = log(GrLivArea), fill = Neighborhood)) + geom_histogram(bins =20)

train_a1 %>% ggplot(aes(x = log(GrLivArea), y = SalePrice/1000, color = Neighborhood)) + geom_point() + stat_smooth(method = "lm")

train_a1 %>% ggplot(aes(x = (GrLivArea), y = log(SalePrice/1000), color = Neighborhood)) + geom_point() + stat_smooth(method = "lm")

train_a1 %>% ggplot(aes(x = log(GrLivArea), y = log(SalePrice/1000), color = Neighborhood)) + geom_point() + stat_smooth(method = "lm")
```
Using a log transform on the GrLivArea and SalePrice variables, we achieve a more normal distribution for each variable which will be more conducive to use in our model. 

From visual inspection, we can identify that there is a visual difference in slope/intercept values per group, and can use methods to identify and quantify what these values are. 

```{r}
#Comparison of Means/Coefficients to signify a difference in sample groups from different neighborhoods.
### comparison of means for the groups
n_means <- aov(SalePrice ~ Neighborhood, data = a1_set)
mean_diff <- TukeyHSD(n_means, conf.level = 0.95)
mean_diff

### linear model with interactions explained per group
fit <- lm(SalePrice ~ log(GrLivArea) + Neighborhood + GrLivArea*Neighborhood, data = a1_set)
summary(fit)

#RMSE: 
ModelRMSE(fit$residuals)
```
## Linear Models

From a visual inspection of the Cook's D and residual plot, we notice there are 3 outliers: 
  - ID 54
  - ID 61, 
  - ID 114
```{r}
fit1 <- lm(SalePrice ~ GrLivArea + Neighborhood + GrLivArea*Neighborhood, data = a1_set)
summary(fit1)
confint(fit1)

plot(fit1)

hist(fit1$residuals, col = "blue", main = "Histogram of Residuals")
sqrt(mean(fit1$residuals^2)) #RMSE
```

### Remove Outliers 
```{r}
head(a1_set)
a1_set2 = housing
a1_set2 = a1_set2 [(!(a1_set2 $Id == "54") & !(a1_set2 $Id == "61") & !(a1_set2 $Id == "114") ),]
head(a1_set2)

a1_set2 = a1_set2 %>% select(SalePrice, Neighborhood, GrLivArea) %>% filter( Neighborhood == c("NAmes", "Edwards", "BrkSide"))
head(a1_set2)
```

### Fit model without outliers 
```{r}
fit2 <- lm(log(SalePrice) ~ log(GrLivArea) + Neighborhood + GrLivArea*Neighborhood, data = a1_set2)
summary(fit2)
confint(fit2)

plot(fit2)

hist(fit2$residuals, col = "blue", main = "Histogram of Residuals")
sqrt(mean(fit2$residuals^2)) #RMSE
```



Analysis 2: 


For Analysis 2, we need to generate 4 models using variations of parameters selected by Forwards Parameter Selection, Backwards Elimination, StepWise Selection, and a 
personalized custom approach. 

Using these paramters, we can generate Adjusted R^2 values, CV Press statistics, and finally submit our model to the Kaggle website to obtain a "Kaggle Score" for each model. 

This process begins as follows: 

Rough approach to Forwards Selection / Backwards Elimination / Stepwise Selection model generation: 

```{r}
#bad approach, maybe, I can't get it to work 
train.control = trainControl(method = "cv", number = 10)
step.model = train(SalePrice ~.-GarageArea, data = housing %>% select_if(is.numeric) %>% drop_na(), method = "leapBackward", trControl = train.control)
pred(step.model, housing)
#step.model$modelInfo
```

For the process of forward feature selection or backwards feature elimination, we can use the `olsrr` package to do most of the selection work for us. This can be seen as follows: 

```{r}
install.packages("olsrr")

#turning all character columns into factors for LR: 
character_names = housing %>% select_if(is.character) %>% names()
housing[character_names] = lapply(housing[character_names], factor)

fit_numeric = lm(SalePrice ~., data = housing %>% select_if(is.numeric)) # does not account for categorical parameters !!!

#forwards selection: 
forward = ols_step_forward_p(fit_numeric, penter = 0.05, details = FALSE)
#backwards
backwards = ols_step_backward_p(fit_numeric, prem = 0.05, details = FALSE)
#stepwise
stepwise = ols_step_both_p(fit_numeric, pent = 0.05, prem =0.05, details = FALSE)
```

From these methods we can pull our the corresponding models and analyze the statistics as well as the parameters that we chosen, to verify that first, there was a reduction in selection in the parameters for the LM and second, 


This step might not be necessary but could improve the quality of the model. Analysis shows a marginal improvement to R^2 and marginal deterioration to PRESS Stat when rm'ing outliers. 

Outlier ID# : 692, 1183,  Excessively Expensive > 700k
Outlier ID# : 314, 336, 256, 707    Excessively Large > 100k sqr ft. 


```{r}
#I'm not sure if this is the best way to do it, but it highlights the requirement of the transform

boxplot(numeric_housing) #<-visualize outliers

#filtered_nh = numeric_housing[-c(692, 1183, 314, 336, 256, 707), ] #filters out outliers 
#filtered_nh$SalePrice = log(filtered_nh$SalePrice) 
#filtered_nh$LotArea = log(filtered_nh$LotArea)

transformed_numeric = numeric_housing
transformed_numeric$SalePrice = log(transformed_numeric$SalePrice)
transformed_numeric$LotArea = log(transformed_numeric$LotArea)
transformed_numeric$GrLivArea = log(transformed_numeric$GrLivArea)

boxplot(transformed_numeric)

transformed_fit = lm(SalePrice ~., data = transformed_numeric)

transForward = ols_step_forward_p(transformed_fit, penter = 0.05, details = FALSE)

transForward
summary(transForward$model)
ModelRMSE(transForward$model$residuals) #see defined functions at top of document
CVPress(transForward$model)

#boxplot(filtered_nh)

#filtered_fit_numeric = lm(SalePrice ~., data = filtered_nh)                               #outlier filtered approach

#f_forward = ols_step_forward_p(filtered_fit_numeric, penter = 0.05, details = FALSE)

#ModelRMSE(f_forward$model$residuals) #see defined functions at top of document
#CVPress(f_forward$model)
```

```{r Reduced Linearity Set}
t1 = subset(transformed_numeric, select = -c(LotFrontage, MasVnrArea, GarageYrBlt, OverallQual, TotRmsAbvGrd, GarageCars, TotalBsmtSF, X1stFlrSF, X2ndFlrSF, FullBath )) 
#removal of largely correlated values to reduce multicollinearity (VIF /approx 1) and actually allow back-elimination to run,
#it wasn't running without a ton of errors without this. 

corrplot(cor(t1)) #visualize correlation to identify which plots are correlated. After elimiation, it should be fairly sparsely correlated. 

t1fit = lm(SalePrice ~. , data = t1)

backt1 = ols_step_backward_p(t1fit, prem = 0.05, details = FALSE)
forwardt1 = ols_step_forward_p(t1fit, penter = 0.05, details = FALSE)               #the p-values for these should be fiddled with, 
stepwiset1 = ols_step_both_p(t1fit, pent = 0.05, prem =0.05, details = FALSE)

forwardt1
backt1
stepwiset1
#mystery4thsetWIP

#---------------------------------Summary Stats of the produced RL sets
summary(backt1$model)
vif(backt1$model)

ModelRMSE(backt1$model$residuals)
CVPress(backt1$model)
#---------------------------------
summary(forwardt1$model)
vif(forwardt1$model)

ModelRMSE(forwardt1$model$residuals)
CVPress(forwardt1$model)
#---------------------------------
summary(stepwiset1$model)
vif(stepwiset1$model)

ModelRMSE(stepwiset1$model$residuals)
CVPress(stepwiset1$model)


```
## Custom Model 

```{r}
# Change categorical values to numeric
housing2 = housing
housing2$Neighborhood <- as.numeric(factor(housing2$Neighborhood,
                                                        levels = c("Blmngtn", "Blueste", "BrDale"),
                                                        labels = c(2,1,0), ordered = TRUE))
housing2$LotConfig <- as.numeric(factor(housing2$LotConfig,
                                                        levels = c("Inside", "Corner", "CulDSac", "FR2", "FR3"),
                                                        labels = c(4,3,2,1,0), ordered = TRUE))
housing2$ExterQual <- as.numeric(factor(housing2$ExterQual,
                                                        levels = c("Ex", "Gd", "TA", "Fa", "Po"),
                                                        labels = c(4, 3, 2,1,0), ordered = TRUE))
housing2$BsmtQual <- as.numeric(factor(housing2$BsmtQual,
                                                        levels = c("Ex", "Gd", "TA", "Fa", "Po"),
                                                        labels = c(4, 3, 2,1,0), ordered = TRUE))


```
```{r}
# Only look at Normal Sales 
normal_housing = housing 
normal_set = normal_housing %>% select(SaleCondition) %>% filter( SaleCondition == c("Normal"))
normal_housing = normal_housing %>% select_if(is.numeric)

normal_housing$SalePrice = log(normal_housing$SalePrice)
normal_housing$GrLivArea = log(normal_housing$GrLivArea)

```

```{r}
t2 = subset(normal_housing, select = -c(LotFrontage, MasVnrArea, GarageYrBlt, OverallQual, TotRmsAbvGrd, GarageCars, TotalBsmtSF, X1stFlrSF, X2ndFlrSF, FullBath )) 
#removal of largely correlated values to reduce multicollinearity (VIF /approx 1) and actually allow back-elimination to run,
#it wasn't running without a ton of errors without this. 

corrplot(cor(t2)) #visualize correlation to identify which plots are correlated. After elimiation, it should be fairly sparsely correlated. 

t2fit = lm(SalePrice ~. , data = t2)

backt2 = ols_step_backward_p(t2fit, prem = 0.05, details = FALSE)
forwardt2 = ols_step_forward_p(t2fit, penter = 0.05, details = FALSE)               #the p-values for these should be fiddled with, 
stepwiset2 = ols_step_both_p(t2fit, pent = 0.05, prem =0.05, details = FALSE)

forwardt2
backt2
stepwiset2
#mystery4thsetWIP

#---------------------------------Summary Stats of the produced RL sets
summary(backt2$model)
vif(backt2$model)

ModelRMSE(backt2$model$residuals)
CVPress(backt2$model)
#---------------------------------
summary(forwardt2$model)
vif(forwardt2$model)

ModelRMSE(forwardt2$model$residuals)
CVPress(forwardt2$model)
#---------------------------------
summary(stepwiset2$model)
vif(stepwiset2$model)

ModelRMSE(stepwiset2$model$residuals)
CVPress(stepwiset2$model)

```


Now that we have generated 3 models (Need 4!!!) we can predict the saleprice on a test set of data, submit this modelled set to Kaggle to generate a Kaggle Score for the model and analyze whether our approach was correct or not. That is done as following; 

```{r}

# build predictions from set: 

#since the original dataset is transformed, the testset needs to be transformed
#as well to account for the change in base.
test_housing$LotArea = log(test_housing$LotArea)
test_housing$GrLivArea = log(test_housing$GrLivArea)

#--------------------------------------------- Backwards Elimination Prediction: 

back_pred = predict(backt1$model, test_housing) 

#Since the original dataset was transformed, we need to counter-transform to reobtain the
#untransformed data. 

real_bp = exp(back_pred)

#combine with ID and export as .csv

real_bp_joined = data.frame(test_housing$Id, real_bp)
colnames(real_bp_joined) = c("Id", "SalePrice")
real_bp_joined[is.na(real_bp_joined)]<-10000 #if there are any NA, set them to arbitrary 10k price. 


write.csv(real_bp_joined, "./Kaggle_backt1_testset.csv", row.names = FALSE)

#-------------------------------------------- Forwards Selection Prediction: 

forward_pred = predict(forwardt1$model, test_housing)
real_forward = exp(forward_pred)
real_fp_joined = data.frame(test_housing$Id, real_forward)
real_fp_joined[is.na(real_fp_joined)]<-10000
colnames(real_fp_joined) = c("Id", "SalePrice")
write.csv(real_bp_joined, "./Kaggle_forwardt1_testset.csv", row.names = FALSE)

#-------------------------------------------- Stepwise Selection Prediction: 

stepwise_pred = predict(stepwiset1$model, test_housing)
real_step = exp(stepwise_pred)
real_sp_joined = data.frame(test_housing$Id, real_step)
real_sp_joined[is.na(real_sp_joined)]<-10000
colnames(real_sp_joined) = c("Id", "SalePrice")
write.csv(real_sp_joined, "./Kaggle_stepwiset1_testset.csv", row.names = FALSE)


#-------------------------------------------- Custom Model Prediction: 

# Backwards Elimination
test_housing$GrLivArea = log(test_housing$GrLivArea)
back_pred2 = predict(backt2$model, test_housing) 
head(back_pred2)
#Since the original dataset was transformed, we need to counter-transform to reobtain the
#untransformed data. 

real_bp2 = exp(back_pred2)

#combine with ID and export as .csv

real_bp_joined2 = data.frame(test_housing$Id, real_bp2)
colnames(real_bp_joined2) = c("Id", "SalePrice")
real_bp_joined2[is.na(real_bp_joined2)]<-10000 #if there are any NA, set them to arbitrary 10k price. 

head(real_bp_joined2)
write.csv(real_bp_joined2, "./Kaggle_backt2_testset.csv", row.names = FALSE)

# Forward Selection
forward_pred2 = predict(forwardt2$model, test_housing)
real_forward2 = exp(forward_pred2)
real_fp_joined2 = data.frame(test_housing$Id, real_forward2)
real_fp_joined2[is.na(real_fp_joined2)]<-10000
colnames(real_fp_joined2) = c("Id", "SalePrice")
write.csv(real_bp_joined2, "./Kaggle_forwardt2_testset.csv", row.names = FALSE)

# Stepwise Selection 
stepwise_pred2 = predict(stepwiset2$model, test_housing)
real_step2 = exp(stepwise_pred2)
real_sp_joined2 = data.frame(test_housing$Id, real_step2)
real_sp_joined2[is.na(real_sp_joined2)]<-10000
colnames(real_sp_joined2) = c("Id", "SalePrice")
write.csv(real_sp_joined2, "./Kaggle_stepwiset2_testset.csv", row.names = FALSE)

```

The rudimentary approach of numeric/reduced linearity set receives a Kaggle score of 0.20141. While is is interesting and good given the novel approach of the selection model, it's troubling to see that all 3 models returned the exact same Kaggle score, down to the last decimal. This may indicate that the models are adding/subtracting parameters to result in the exact same parameter list each time. This should be further investigated. 


