Analysis 1: 


For the first analysis, we are looking at houses are are only within the locale of [NAmes, Edwards, BrkSide neighborhoods] and at how Sale Price can possibly change
with respect to the Size of the living area and the Neighborhood in which the property is located, which can be visualized with the equation: 

SalePrice ~ SqFt (GrLIvArea) + Neighborhood + GrLIvArea:Neighborhood


Pulling the data into R and performing some rudimentary cleaning; 

```{r include=FALSE}
library(tidyverse)
library(plotly)
library(car)
library(caret)
library(olsrr)
library(corrplot)
```

Defining functions for future use, I didn't know where to put this: 

```{r RMSE function}
ModelRMSE <- function(res){
  retRMSE <- sqrt(mean(res^2))
  return(retRMSE)
}
```


```{r CVPress function}
CVPress <- function(input_model){
  pred_res = resid(input_model)/(1 - lm.influence(input_model)$hat)
  press <- sum(pred_res^2)
  return(press)
}
```


```{r}
housing = read.csv(file = "train.csv")
test_housing = read.csv(file = "test.csv")

numeric_housing = housing %>% select_if(is.numeric)
a1_set = housing %>% select(SalePrice, Neighborhood, GrLivArea) %>% filter( Neighborhood == c("NAmes", "Edwards", "BrkSide"))
```

We can take a look at the dataset and visualize what the datalooks like with some cursory visuals. During this step, we want to analyze any heavy correlation that the dataset might have, and account for it in any of our developing models;  

```{r}

a1_set %>% ggplot(aes(x = (GrLivArea), y = SalePrice/1000, color = Neighborhood)) + geom_point() + stat_smooth(method = "lm")

a1_set %>% ggplot(aes(x = SalePrice/10000, fill = Neighborhood)) + geom_histogram(bins =20)
```

From visual inspection, we can identify that there is a visual difference in slope/intercept values per group, and can use methods to identify and quantify what these values are. 

```{r}
#Comparison of Means/Coefficients to signify a difference in sample groups from different neighborhoods.
### comparison of means for the groups
n_means <- aov(SalePrice ~ Neighborhood, data = a1_set)
mean_diff <- TukeyHSD(n_means, conf.level = 0.95)
mean_diff

### linear model with interactions explained per group
fit <- lm(SalePrice ~ log(GrLivArea) + Neighborhood + GrLivArea*Neighborhood, data = a1_set)
summary(fit)

#RMSE: 
ModelRMSE(fit$residuals)
```





Analysis 2: 


For Analysis 2, we need to generate 4 models using variations of parameters selected by Forwards Parameter Selection, Backwards Elimination, StepWise Selection, and a 
personalized custom approach. 

Using these paramters, we can generate Adjusted R^2 values, CV Press statistics, and finally submit our model to the Kaggle website to obtain a "Kaggle Score" for each model. 

This process begins as follows: 

Rough approach to Forwards Selection / Backwards Elimination / Stepwise Selection model generation: 

```{r}
#bad approach, maybe, I can't get it to work 
#train.control = trainControl(method = "cv", number = 10)
#step.model = train(SalePrice ~.-GarageArea, data = housing %>% select_if(is.numeric) %>% drop_na(), method = "leapBackward", trControl = train.control)
#step.model$modelInfo
```

For the process of forward feature selection or backwards feature elimination, we can use the `olsrr` package to do most of the selection work for us. This can be seen as follows: 

```{r}
#install.packages("olsrr")

#turning all character columns into factors for LR: 
character_names = housing %>% select_if(is.character) %>% names()
housing[character_names] = lapply(housing[character_names], factor)

fit_numeric = lm(SalePrice ~., data = housing %>% select_if(is.numeric)) # does not account for categorical parameters !!!

#forwards selection: 
forward = ols_step_forward_p(fit_numeric, penter = 0.05, details = FALSE)
#backwards
backwards = ols_step_backward_p(fit_numeric, prem = 0.05, details = FALSE)
#stepwise
stepwise = ols_step_both_p(fit_numeric, pent = 0.05, prem =0.05, details = FALSE)
```

From these methods we can pull our the corresponding models and analyze the statistics as well as the parameters that we chosen, to verify that first, there was a reduction in selection in the parameters for the LM and second, 


This step might not be necessary but could improve the quality of the model. Analysis shows a marginal improvement to R^2 and marginal deterioration to PRESS Stat when rm'ing outliers. 

Outlier ID# : 692, 1183,  Excessively Expensive > 700k
Outlier ID# : 314, 336, 256, 707    Excessively Large > 100k sqr ft. 


```{r}
#I'm not sure if this is the best way to do it, but it highlights the requirement of the transform

boxplot(numeric_housing) #<-visualize outliers

#filtered_nh = numeric_housing[-c(692, 1183, 314, 336, 256, 707), ] #filters out outliers 
#filtered_nh$SalePrice = log(filtered_nh$SalePrice) 
#filtered_nh$LotArea = log(filtered_nh$LotArea)

transformed_numeric = numeric_housing
transformed_numeric$SalePrice = log(transformed_numeric$SalePrice)
transformed_numeric$LotArea = log(transformed_numeric$LotArea)

boxplot(transformed_numeric)

transformed_fit = lm(SalePrice ~., data = transformed_numeric)

transForward = ols_step_forward_p(transformed_fit, penter = 0.05, details = FALSE)

summary(transForward$model)
ModelRMSE(transForward$model$residuals) #see defined functions at top of document
CVPress(transForward$model)

#boxplot(filtered_nh)

#filtered_fit_numeric = lm(SalePrice ~., data = filtered_nh)                               #outlier filtered approach

#f_forward = ols_step_forward_p(filtered_fit_numeric, penter = 0.05, details = FALSE)

#ModelRMSE(f_forward$model$residuals) #see defined functions at top of document
#CVPress(f_forward$model)
```

```{r Reduced Linearity Set}
t1 = subset(transformed_numeric, select = -c(LotFrontage, MasVnrArea, GarageYrBlt, OverallQual, TotRmsAbvGrd, GarageCars, TotalBsmtSF, X1stFlrSF, X2ndFlrSF, FullBath )) 
#removal of largely correlated values to reduce multicollinearity (VIF /approx 1) and actually allow back-elimination to run,
#it wasn't running without a ton of errors without this. 

corrplot(cor(t1)) #visualize correlation to identify which plots are correlated. After elimiation, it should be fairly sparsely correlated. 

t1fit = lm(SalePrice ~. , data = t1)

backt1 = ols_step_backward_p(t1fit, prem = 0.05, details = FALSE)
forwardt1 = ols_step_forward_p(t1fit, penter = 0.05, details = FALSE)               #the p-values for these should be fiddled
stepwiset1 = ols_step_both_p(t1fit, pent = 0.05, prem =0.05, details = FALSE)       #with to test a better selection set, although could be unnec. 

#---------------------------------Summary Stats of the produced RL sets
summary(backt1$model)
vif(backt1$model)

ModelRMSE(backt1$model$residuals)
CVPress(backt1$model)
#---------------------------------
summary(forwardt1$model)
vif(forwardt1$model)

ModelRMSE(forwardt1$model$residuals)
CVPress(forwardt1$model)
#---------------------------------
summary(stepwiset1$model)
vif(stepwiset1$model)

ModelRMSE(stepwiset1$model$residuals)
CVPress(stepwiset1$model)


```

Now that we have generated 3 models (Need 4!!!) we can predict the saleprice on a test set of data, submit this modelled set to Kaggle to generate a Kaggle Score for the model and analyze whether our approach was correct or not. That is done as following; 

```{r}

# build predictions from set: 

#since the original dataset is transformed, the testset needs to be transformed
#as well to account for the change in base.
test_housing$LotArea = log(test_housing$LotArea)

back_pred = predict(backt1$model, test_housing) 

#Since the original dataset was transformed, we need to counter-transform to reobtain the
#untransformed data. 

real_bp = exp(back_pred)

#combine with ID and export as .csv

real_bp_joined = data.frame(test_housing$Id, real_bp)
colnames(real_bp_joined) = c("Id", "SalePrice")
real_bp_joined[is.na(real_bp_joined)]<-10000 #if there are any NA, set them to arbitrary 10k price. 


write.csv(real_bp_joined, "./Kaggle_backt1_testset.csv", row.names = FALSE)

```

The rudimentary approach of numeric/reduced linearity set receives a Kaggle score of 0.20


